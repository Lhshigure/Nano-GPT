import math
import numpy as np
import torch
from Nanogpt_modified import GPT, GPTConfig
from utilities import iterate_examples, render_example, get_most_likely_row
import warnings
import tiktoken

warnings.filterwarnings("ignore", category=FutureWarning)

def generate_sample(model, tokenizer, prompt, max_new_tokens=30, device='cuda'):
    model.eval()
    tokens = torch.tensor(tokenizer.encode(prompt), dtype=torch.long).unsqueeze(0).to(device)

    eos_id = getattr(tokenizer, "eos_token_id", None)
    if eos_id is None:
        eos_id = getattr(tokenizer, "eot_token", None)  # tiktoken å¸¸è§
    if eos_id is None:
        eos_id = 50256  # GPT-2 endoftext å…œåº•

    with torch.no_grad():
        for _ in range(max_new_tokens):
            idx_cond = tokens[:, -model.config.block_size:]
            logits, _ = model(idx_cond)
            logits = logits[:, -1, :]
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            tokens = torch.cat((tokens, next_token), dim=1)
            if next_token.item() == eos_id:
                break

    out = tokenizer.decode(tokens[0].tolist())
    model.train()
    return out

def load_tokens(filename):
    npt = np.fromfile(filename, dtype=np.uint16) # è¯»å–åŸå§‹äºŒè¿›åˆ¶
    ptt = torch.tensor(npt.astype(np.int64), dtype=torch.long) # è½¬ä¸º long
    return ptt

class DataLoaderLite:
    def __init__(self, B, T, process_rank, num_processes, split):
        self.B = B
        self.T = T
        self.process_rank = process_rank
        self.num_processes = num_processes

        assert split in {'train', 'val'}

        # get the shard filenames
        data_root = "edu_fineweb10B"
        shards = os.listdir(data_root)
        shards = [s for s in shards if split in s]
        shards = sorted(shards)
        shards = [os.path.join(data_root, s) for s in shards]
        self.shards = shards
        assert len(shards) > 0, f"no shards found for split {split}"
        if master_process:
            print(f"found {len(shards)} shards for split {split}")

        # state, init at shard zero
        self.current_shard = 0
        self.tokens = load_tokens(self.shards[self.current_shard])
        self.current_position = self.B * self.T * self.process_rank
    
    def reset(self):
        # state init at shard zero
        self.current_shard = 0
        self.tokens = load_tokens(self.shards[self.current_shard])
        self.current_position = self.B * self.T * self.process_rank

    def next_batch(self):
        B, T = self.B, self.T
        buf = self.tokens[self.current_position : self.current_position + B*T + 1]
        x = (buf[:-1]).view(B, T) # inputs
        y = (buf[1:]).view(B, T)  # targets
        self.current_position += B * T *self.num_processes
        # out of bounds, rest
        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
            self.current_shard = (self.current_shard + 1) % len(self.shards)
            self.tokens = load_tokens(self.shards[self.current_shard])
            self.current_position = self.B * self.T * self.process_rank
        return x.to(device), y.to(device)
#---------------------------------------------------------------------------------------------------
# simple run:
# python train_gpt2.py
# DDP run:
# torchrun --standalone --nproc_per_node=8 train_gpt2.py

import time
import os
# å¤šå¡GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
device_type = "cuda" if "cuda" in device else "cpu"
num_return_sequences = 5
max_length = 30

# run the training loop
from torch.distributed import init_process_group, destroy_process_group
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.distributed as dist
# è®¾ç½® DDP (Distributed Data Parallel)
# torchrun å‘½ä»¤ä¼šè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ RANK, LOCAL_RANK, å’Œ WORLD_SIZE
ddp = int(os.environ.get('RANK', -1)) != -1 # è¿™æ˜¯ä¸€ä¸ª ddp è¿è¡Œå—ï¼Ÿ
if ddp:
    # ä½¿ç”¨ DDP æ¨¡å¼
    assert torch.cuda.is_available(), "for now i think we need CUDA for DDP"
    init_process_group(backend='nccl')
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = f'cuda:{ddp_local_rank}'
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0 # è¿™ä¸ªè¿›ç¨‹å°†è´Ÿè´£æ—¥å¿—è®°å½•ã€ä¿å­˜æ£€æŸ¥ç‚¹ç­‰
else:
    # æ™®é€šçš„å•å¡è¿è¡Œæ¨¡å¼
    ddp_rank = 0
    ddp_local_rank = 0
    ddp_world_size = 1
    master_process = True
    # å°è¯•è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        device = "mps"
    print(f"using device: {device}")


torch.manual_seed(1337)
if torch.cuda.is_available():
    torch.cuda.manual_seed(1337)

total_batch_size = 524288 #    GPT2(124m):2**19, ~0.5m, in number of tokens
B = 16 # micro_step          GPT2(124m):16
T = 1024  #                  GPT2(124m):1024
assert total_batch_size % (B * T * ddp_world_size) == 0, "make sure total_batch_size is divisible by B * T *ddp_world_size"
grad_accum_steps = total_batch_size // (B * T * ddp_world_size)
if master_process:
    print(f"total desired batch size: {total_batch_size}")
    print(f"=> calculated gradient accumulation steps: {grad_accum_steps}")

print("I am GPU", ddp_rank)
print("OK!")

train_loader = DataLoaderLite(B = B, T = T, process_rank=ddp_rank, num_processes=ddp_world_size, split="train")
val_loader = DataLoaderLite(B = B, T = T, process_rank=ddp_rank, num_processes=ddp_world_size, split="val")

torch.set_float32_matmul_precision('high') # mixed precision

#----------------------------------------------------------------------------------------------------------------------------------------------------
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")

config = GPTConfig(
    vocab_size=50304, 
    block_size=T, 
    max_batch_size=B, 
    n_layer=12,
    n_head=12,
    n_kv_heads=12,     # QA è®¾ç½®
    n_embd=768
)
# create model
model = GPT(config)   
model.to(device)
use_compile = True
if use_compile:
    model = torch.compile(model) # model = torch.compile(model) windowä¸‹ä¸å®Œå…¨æ”¯æŒï¼

if ddp:
    model = DDP(model, device_ids=[ddp_local_rank]) # all reduce
raw_model = model.module if ddp else model

# GPT-3 learning rate setting, Dataset: edu_fineweb10B
max_lr = 6e-4
min_lr = max_lr * 0.1
warmup_steps = 712
max_steps = 19073

#----------------------------------------------------------------
# wsd + cosine
def get_lr_wsd_cosine(it, decay_start_pct=0.8):
    # 1. Warmup
    if it < warmup_steps:
        return max_lr * (it + 1) / warmup_steps
    
    # 2. Stable
    decay_start_step = int(max_steps * decay_start_pct)
    if it < decay_start_step:
        return max_lr
    
    # 3. Cosine Cooldown (æ ¸å¿ƒæ”¹åŠ¨)
    decay_steps = max_steps - decay_start_step
    it_in_decay = it - decay_start_step
    # è®¡ç®—ä½™å¼¦ç³»æ•°
    coeff = 0.5 * (1.0 + math.cos(math.pi * it_in_decay / decay_steps))
    return min_lr + coeff * (max_lr - min_lr)


def get_lr_cos(it):
    # 1) linear warmp for warmup_iter steps
    if it < warmup_steps:
        return max_lr * (it + 1) / warmup_steps
    if it > max_steps:
        return min_lr
    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
    assert 0 <= decay_ratio <= 1
    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))
    return min_lr + coeff * (max_lr - min_lr)

# optimizer
# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps = 1e-8)
optimizers = raw_model.configure_optimizers(learning_rate=6e-4, device=device)
opt_muon, opt_adamw = optimizers

#-------------è®°å½•è®­ç»ƒæ•°æ®-----------------------------
# --- æ–°å¢ï¼šæ–­ç‚¹ç»­è®­åŠ è½½é€»è¾‘ ---
import csv
log_dir = "log"  # save file
start_step = 0
os.makedirs(log_dir, exist_ok=True)
checkpoint_path = os.path.join(log_dir, "model_latest.pt")
#ä¿å­˜æœ€ä½³loss model
best_val_loss = float('inf')

if os.path.exists(checkpoint_path):
    if master_process:
        print(f"æ­£åœ¨ä» {checkpoint_path} æ¢å¤è®­ç»ƒ...")
    
    # 1. åŠ è½½åˆ°æ­£ç¡®è®¾å¤‡ 
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    # 2. è·å–çŠ¶æ€å­—å…¸
    state_dict = checkpoint["model"]
    
    # 3. æ¢å¤æƒé‡
    raw_model.load_state_dict(state_dict, strict=False)
    
    # 4. æ¢å¤ä¸¤ä¸ªä¼˜åŒ–å™¨çš„çŠ¶æ€
    # ç¡®ä¿åœ¨ resume ä¹‹å‰å·²ç»åˆå§‹åŒ–å¥½äº† opt_muon å’Œ opt_adamw
    opt_muon.load_state_dict(checkpoint['optimizer_muon'])
    opt_adamw.load_state_dict(checkpoint['optimizer_adamw'])
    
    # 5. æ›´æ–°èµ·å§‹æ­¥æ•°
    start_step = checkpoint['step'] + 1
    
    # 6. æ¢å¤æœ€ä½³éªŒè¯æŸå¤±
    # å¢åŠ  val_loss å­˜åœ¨çš„åˆ¤æ–­ï¼Œé˜²æ­¢æ—§ checkpoint æ ¼å¼æŠ¥é”™
    if 'val_loss' in checkpoint and checkpoint['val_loss'] is not None:
        best_val_loss = checkpoint['val_loss']
    
    if master_process:
        print(f"âœ… æ–­ç‚¹æ¢å¤æˆåŠŸï¼KV-Cache å·²é‡ç½®ä¸ºå…¨é›¶ï¼Œå°†ä» Step {start_step} ç»§ç»­è®­ç»ƒã€‚")
    

csv_log_path = os.path.join(log_dir, "training_stats.csv")
if master_process:

    if start_step == 0:
        with open(csv_log_path, "w", newline="") as f:
            writer = csv.writer(f)
            # æ ¸å¿ƒæŒ‡æ ‡ï¼šæ­¥éª¤ã€ä¸¤ç§Lossã€ä¸¤ç§å­¦ä¹ ç‡ã€æ¢¯åº¦èŒƒæ•°ã€ååé‡ã€è¯„æµ‹å‡†ç¡®ç‡
            writer.writerow([
                "step", "train_loss", "val_loss", "val_ppl", 
                "lr_adamw", "lr_muon", "norm", "dt_ms", 
                "tokens_per_sec", "hella_acc"
            ])


#train loop
import contextlib
#----------------------------------------------------------
for step in range(start_step, max_steps):
    t0 = time.time()
    last_step = (step == max_steps - 1)

    # once in a while evaluate our validation loss:
    if step % 250 ==0 or last_step:
        model.eval()
        val_loader.reset()
        with torch.no_grad():
            val_loss_accum = 0.0
            val_loss_steps = 20
            for _ in range(val_loss_steps):
                x, y = val_loader.next_batch()
                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
                    logits, loss = model(x, y)
                loss = loss /val_loss_steps
                val_loss_accum += loss.detach()
        if ddp:
            dist.all_reduce(val_loss_accum, op=dist.ReduceOp.AVG)
        if master_process:
            # è®¡ç®— PPL
            val_ppl = math.exp(val_loss_accum.item())
            print(f"validation loss: {val_loss_accum.item():.4f} | ppl: {val_ppl:.4f}")
    
    # -----------------------------------------------------------------------------
    # HellaSwag è¯„ä¼°é€»è¾‘
    if ((step > 0 and step % 500 == 0) or last_step) and (not use_compile):
        num_correct_norm = 0
        num_total = 0
        
        # iterate_examples("val") ä¼šè¯»å– HellaSwag çš„éªŒè¯é›†æ•°æ®
        for i, example in enumerate(iterate_examples("val")):
            
            # å¤šå¡å¹¶è¡Œ (DDP) é€»è¾‘ï¼šæ¯ä¸ªè¿›ç¨‹åªå¤„ç†è‡ªå·±é‚£éƒ¨åˆ†æ•°æ®
            if i % ddp_world_size != ddp_rank:
                continue
                
            # å°†åŸå§‹ä¾‹å­æ¸²æŸ“ä¸º tokensã€mask å’Œæ­£ç¡®ç­”æ¡ˆ label
            _, tokens, mask, label = render_example(example, tokenizer)
            tokens = tokens.to(device)
            mask = mask.to(device)
            
            # è·å–æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):
                    logits, loss = model(tokens)
                    
                # get_most_likely_row ä¼šå¯¹æ¯” 4 ä¸ªé€‰é¡¹ä¸­å“ªä¸€ä¸ªæ¦‚ç‡æœ€é«˜
                pred_norm = get_most_likely_row(tokens, mask, logits)
                num_total += 1
                num_correct_norm += int(pred_norm == label)
                
        # å¦‚æœæ˜¯å¤šå¡å¹¶è¡Œï¼Œéœ€è¦å°†æ‰€æœ‰è¿›ç¨‹çš„ç»Ÿè®¡ç»“æœç›¸åŠ 
        if ddp:
            num_total = torch.tensor(num_total, dtype=torch.long, device=device)
            num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)
            dist.all_reduce(num_total, op=dist.ReduceOp.SUM)
            dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)
            num_total = num_total.item()
            num_correct_norm = num_correct_norm.item()
            
        # è®¡ç®—å‡†ç¡®ç‡å¹¶æ‰“å°
        acc_norm = num_correct_norm / num_total
        if master_process:
            print(f"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}")

    
    # -----------------------------------------------------------------------------
    # once a while generate from model (except step 0)
    if step % 500 == 0 or last_step:
        if master_process:
            test_prompt = "The most important thing in life is"
            sample_text = generate_sample(raw_model, tokenizer, test_prompt, device=device)
        
            print(f"--- Step {step} Sample ---")
            print(f"Prompt: {test_prompt}")
            print(f"Generated: {sample_text}")
            
            with open(os.path.join(log_dir, "samples.txt"), "a", encoding="utf-8") as f:
                f.write(f"--- STEP {step} ---\n")
                f.write(f"PROMPT: {test_prompt}\n")
                f.write(f"RESULT: {sample_text}\n\n")
    # training loop
    model.train()
    # å¯¹ä¸¤ä¸ªä¼˜åŒ–å™¨éƒ½è¦æ¸…é›¶
    max_lr_muon = 0.02   #Muon å³°å€¼å­¦ä¹ ç‡
    for opt in optimizers:
        opt.zero_grad(set_to_none=True) 

    loss_accum = 0.0
    for micro_step in range(grad_accum_steps):
        x, y = train_loader.next_batch()
        ctx = model.no_sync() if ddp and micro_step < grad_accum_steps - 1 else contextlib.nullcontext()
        with ctx:
            logits, loss = model(x, y)
        
        loss = loss / grad_accum_steps
        loss_accum += loss.detach()
        

        if ddp:
            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)
        
        loss.backward()

    if ddp:
        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)
    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    lr = get_lr_wsd_cosine(step)

    lr_multiplier = lr / max_lr if max_lr > 0 else 0.0  #max AdamW_lr = max_lr
    muon_lr = max_lr_muon * lr_multiplier

    # AdamW å­¦ä¹ ç‡
    for param_group in opt_adamw.param_groups:
        param_group['lr'] = lr

    # Muon å­¦ä¹ ç‡
    for param_group in opt_muon.param_groups:
        param_group['lr'] = muon_lr

    for opt in optimizers:
        opt.step()

    # åŒæ­¥ GPUï¼Œç¡®ä¿æ—¶é—´ç»Ÿè®¡å‡†ç¡®
    if device_type == "cuda":
        torch.cuda.synchronize()
    t1 = time.time()
    dt = (t1 - t0) # time difference in miliseconds
    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size
    tokens_per_sec = tokens_processed / dt
    if master_process:
        print(f"step {step} | loss:{loss_accum.item()} | lr:{lr:.6f} | norm:{norm:.4f} | dt:{dt*1000:2f}ms | tokens/sec:{tokens_per_sec}")

        # ä¿å­˜é—´éš”
        if step > 0 and (step % 1000 == 0 or last_step):
            if master_process:
                # æ„é€  Checkpoint å­—å…¸
                checkpoint = {
                    'model': raw_model.state_dict(),
                    'config': raw_model.config,
                    'step': step,
                    'val_loss': val_loss_accum.item() if 'val_loss_accum' in locals() else None,
                    'optimizer_muon': opt_muon.state_dict(),
                    'optimizer_adamw': opt_adamw.state_dict(),
                }
                
                # 1. å§‹ç»ˆä¿å­˜ä¸º latestï¼Œç”¨äºä¸‹æ¬¡æ–­ç‚¹ç»­è®­ï¼ˆè‡ªåŠ¨è¦†ç›–æ—§çš„ latestï¼‰
                latest_path = os.path.join(log_dir, "model_latest.pt")
                torch.save(checkpoint, latest_path)
                print(f"ğŸ’¾ å·²æ›´æ–°æœ€æ–°è¿›åº¦è‡³ {latest_path}")

        
        # å‡†å¤‡æœ¬æ­¥è¦è®°å½•çš„æ•°æ®
        # å‡†å¤‡è¯„ä¼°æ•°æ®ï¼šåªæœ‰åœ¨ç‰¹å®šæ­¥æ•°æ‰å¡«å…¥æ•°å€¼ï¼Œå¦åˆ™ç•™ç©º ""
        is_val_step = (step % 250 == 0 or last_step)
        is_hella_step = ((step > 0 and step % 500 == 0) or last_step)
        
        v_loss = val_loss_accum.item() if is_val_step else ""
        v_ppl = math.exp(val_loss_accum.item()) if is_val_step else ""
        h_acc = acc_norm if (is_hella_step and 'acc_norm' in locals()) else ""

        with open(csv_log_path, "a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                step, 
                loss_accum.item(), 
                v_loss, 
                v_ppl,           
                lr, 
                muon_lr, 
                norm.item(), 
                dt * 1000,       
                tokens_per_sec, 
                h_acc            
            ])

if ddp:
    destroy_process_group()