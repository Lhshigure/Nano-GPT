{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0874346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œå¦‚çº¿æ€§å±‚ (Linear)ã€åµŒå…¥å±‚ (Embedding) ç­‰\n",
    "import torch.nn.functional as F  # æ“ä½œçš„å‡½æ•°å¼æ¥å£ï¼Œå¦‚äº¤å‰ç†µ (cross_entropy)ã€SiLU æ¿€æ´»å‡½æ•°ç­‰\n",
    "from torch.utils.data import Dataset, DataLoader  # åŠ è½½æ•°æ®é›†çš„åŸºç¡€ç±»å’Œå®ç”¨å·¥å…·\n",
    "from torch.cuda.amp import autocast, GradScaler  # è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP) å·¥å…·ï¼Œç”¨äºå®ç°æ›´å¿«ã€æ›´ä½æ˜¾å­˜å ç”¨çš„è®­ç»ƒ\n",
    "\n",
    "import math  # æ ‡å‡†æ•°å­¦è¿ç®—ï¼ˆä¾‹å¦‚å¹³æ–¹æ ¹ sqrtã€æŒ‡æ•° expã€ä½™å¼¦ cosï¼‰\n",
    "import random  # Python çš„éšæœºæ•°ç”Ÿæˆå·¥å…·ï¼ˆç”¨äºè®¾ç½®éšæœºç§å­ï¼‰\n",
    "import numpy as np  # æ•°å€¼è®¡ç®—åº“ï¼Œç”¨äºéšæœºç§å­è®¾ç½®å’Œé€šç”¨çš„æ•°ç»„æ“ä½œ\n",
    "\n",
    "from datasets import load_dataset  # Hugging Face çš„æ•°æ®é›†åº“ï¼Œç”¨äºæµå¼åŠ è½½å¤§è§„æ¨¡æ•°æ®é›†\n",
    "from tqdm import tqdm  # è¿›åº¦æ¡å¯è§†åŒ–åº“ï¼Œéå¸¸é€‚åˆå¾ªç¯è¿‡ç¨‹\n",
    "\n",
    "import time  # æ—¶é—´å·¥å…·ï¼Œç”¨äºæµ‹é‡æ—¶é•¿\n",
    "from transformers import AutoTokenizer  # ä»…éœ€ä¸€è¡Œä»£ç å³å¯ä» HuggingFace åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨ (Tokenizer)\n",
    "\n",
    "from dataclasses import dataclass  # å®šä¹‰ç®€å•çš„é…ç½®ç±» (Configs)ï¼Œå‡å°‘å†—ä½™ä»£ç \n",
    "from typing import List, Optional  # ç±»å‹æç¤º (Type hints)ï¼Œæé«˜ä»£ç å¯è¯»æ€§å’Œå·¥å…·æ”¯æŒ\n",
    "\n",
    "import warnings  # æŠ‘åˆ¶æˆ–å¤„ç†è­¦å‘Šä¿¡æ¯\n",
    "import os  # æ–‡ä»¶ç³»ç»Ÿæ“ä½œ\n",
    "import pickle  # Python å¯¹è±¡åºåˆ—åŒ–å·¥å…·ï¼ˆç”¨äºä¿å­˜/åŠ è½½é¢„å¤„ç†åçš„æ•°æ®é›†ï¼‰\n",
    "\n",
    "warnings.filterwarnings('ignore')  # å¿½ç•¥è­¦å‘Šï¼Œä½¿è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡ºæ›´åŠ æ•´æ´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "872b839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Set all seed to {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aed90550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # Model architecture\n",
    "    d_model: int= 384\n",
    "    n_heads: int = 8\n",
    "    n_layers: int = 6\n",
    "    d_ff: int = 1536\n",
    "    batch_size: int = 24\n",
    "    max_steps: int = 500\n",
    "\n",
    "    # Qwen3-like parameters\n",
    "    n_kv_heads: int = 4\n",
    "    sliding_window: int = 4096\n",
    "    attention_bias: bool = False\n",
    "    rms_norm_eps: float = 1e-6\n",
    "\n",
    "    #Training parameters\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    muon_lr: float = 0.01\n",
    "\n",
    "    # Data parameters\n",
    "    max_seq_len: int = 256\n",
    "    num_documents: int = 2000\n",
    "    max_tokens: int = 500000\n",
    "\n",
    "    #Evaluation\n",
    "    eval_every: int = 100\n",
    "    eval_steps: int = 50\n",
    "\n",
    "    # Regularization\n",
    "    weight_decay: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    #Technical\n",
    "    use_amp: bool = True\n",
    "    vocab_size: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_k = self.d_model // self.n_heads\n",
    "        assert self.d_model % self.n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        assert self.n_heads % self.n_kv_heads == 0, \"n_heads must must be divisible by n_kv_heads\"\n",
    "        self.n_kv_groups = self.n_heads // self.n_kv_heads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9365a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GQA helper\n",
    "\n",
    "def repeat_kv(hidden_state: torch.Tensor, n_rep: int)-> torch.Tensor:\n",
    "    \"\"\"\n",
    "    (batch, num_kv_heads, seqLen, head_dim) -> (bath, n_heads, seqLen, head_dim)\n",
    "    \"\"\" \n",
    "\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_state.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_state\n",
    "    hidden_state = hidden_state[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_state.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efe0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int = 5) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    é€šè¿‡ Newton-Schulz è¿­ä»£è®¡ç®—çŸ©é˜µçš„é›¶æ¬¡å¹‚ï¼Œå®ç° G çš„æ ‡å‡†æ­£äº¤åŒ–ï¼ˆOrthogonalizationï¼‰ã€‚\n",
    "    è¯¥å‡½æ•°ä¼šå°†è¾“å…¥çŸ©é˜µ G è½¬æ¢ä¸ºä¸€ä¸ªè¡Œä¸è¡Œï¼ˆæˆ–åˆ—ä¸åˆ—ï¼‰ä¹‹é—´äº’ç›¸å‚ç›´ä¸”é•¿åº¦ä¸º 1 çš„çŸ©é˜µã€‚\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2\n",
    "    # a, b, c æ˜¯ 5 é˜¶ Newton-Schulz è¿­ä»£çš„ç‰¹å®šç³»æ•°ï¼Œç”¨äºå¿«é€Ÿé€¼è¿‘æ­£äº¤çŸ©é˜µ\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "\n",
    "    # å¦‚æœè¡Œæ•°å¤§äºåˆ—æ•°ï¼Œå…ˆè¿›è¡Œè½¬ç½®ï¼Œç¡®ä¿å¯¹è¾ƒå°çš„ç»´åº¦è¿›è¡Œè®¡ç®—ä»¥æé«˜æ•ˆç‡\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # åˆå§‹å½’ä¸€åŒ–ï¼šé™¤ä»¥ Frobenius èŒƒæ•°ï¼Œå°†çŸ©é˜µç¼©æ”¾åˆ°è¿­ä»£æ”¶æ•›èŒƒå›´å†…ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "\n",
    "    # Newton-Schulz æ ¸å¿ƒå¾ªç¯\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT          # è®¡ç®— X ä¹˜ä»¥è‡ªèº«çš„è½¬ç½®ï¼Œè¡¡é‡å…¶ä¸å•ä½çŸ©é˜µçš„è·ç¦»\n",
    "        B = b * A + c * A @ A # è®¡ç®—å¤šé¡¹å¼çš„äºŒé˜¶å’Œå››é˜¶é¡¹\n",
    "        X = a * X + B @ X     # æ›´æ–° Xï¼Œä½¿å…¶è¿›ä¸€æ­¥é€¼è¿‘æ ‡å‡†æ­£äº¤çŸ©é˜µ\n",
    "\n",
    "    # å¦‚æœä¹‹å‰ä¸ºäº†è®¡ç®—æ–¹ä¾¿è½¬ç½®äº†çŸ©é˜µï¼Œç°åœ¨å°†å…¶è½¬ç½®å›åŸå§‹å½¢çŠ¶\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    return X\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon ä¼˜åŒ–å™¨ - é€šè¿‡ Newton-Schulz è¿­ä»£å®ç°åŠ¨é‡æ­£äº¤åŒ– (MomentUm Orthogonalized by Newton-schulz)\n",
    "    \n",
    "    è¯¥ä¼˜åŒ–å™¨ç‰¹åˆ«é€‚ç”¨äº Transformer çš„çº¿æ€§å±‚ï¼Œé€šè¿‡å¼ºåˆ¶æ›´æ–°æ–¹å‘çš„æ­£äº¤æ€§æ¥åŠ é€Ÿæ”¶æ•›ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        # é»˜è®¤è¶…å‚æ•°é…ç½®ï¼šå­¦ä¹ ç‡ã€åŠ¨é‡ã€æ˜¯å¦ä½¿ç”¨ Nesterov åŠ¨é‡ã€Newton-Schulz è¿­ä»£æ­¥æ•°\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"æ‰§è¡Œå•ä¸€çš„ä¼˜åŒ–æ­¥ï¼ˆå‚æ•°æ›´æ–°ï¼‰\"\"\"\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                g = p.grad\n",
    "                state = self.state[p]\n",
    "\n",
    "                # ç¬¬ä¸€æ¬¡è¿è¡Œå‘¨æœŸï¼šåˆå§‹åŒ–åŠ¨é‡ç¼“å­˜ï¼ˆmomentum bufferï¼‰\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "\n",
    "                buf = state[\"momentum_buffer\"]\n",
    "                \n",
    "                # æ›´æ–°åŠ¨é‡ç¼“å­˜ï¼šbuf = momentum * buf + (1 - momentum) * grad\n",
    "                # ä½¿ç”¨ .lerp_ (çº¿æ€§æ’å€¼) æ˜¯ä¸ºäº†åœ¨ GPU ä¸Šè·å¾—æ›´é«˜çš„è®¡ç®—æ•ˆç‡\n",
    "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "                \n",
    "                # å¦‚æœå¯ç”¨ Nesterovï¼Œåˆ™ç»“åˆå½“å‰æ¢¯åº¦å’ŒåŠ¨é‡ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨æ ‡å‡†åŠ¨é‡\n",
    "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                \n",
    "                # æ ¸å¿ƒæ­¥éª¤ï¼šé€šè¿‡ Newton-Schulz è¿­ä»£å°†æ¢¯åº¦è½¬æ¢ä¸ºï¼ˆæ¥è¿‘ï¼‰æ ‡å‡†æ­£äº¤çŸ©é˜µ\n",
    "                # è¿™ä¿è¯äº†æƒé‡çš„æ›´æ–°æ–¹å‘åœ¨æ¯ä¸€ç»´ä¸Šéƒ½æ˜¯ç‹¬ç«‹ä¸”ç­‰æ•ˆçš„\n",
    "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "                \n",
    "                # æ›´æ–°å‚æ•°æƒé‡ï¼š\n",
    "                # è¿™é‡Œåº”ç”¨äº†åŸºäºå‚æ•°å¼ é‡é•¿å®½æ¯”ï¼ˆaspect ratioï¼‰çš„è‡ªé€‚åº”ç¼©æ”¾ã€‚\n",
    "                # å¯¹äºé«˜å¤§äºå®½çš„çŸ©é˜µï¼ˆheight > widthï¼‰ï¼Œå­¦ä¹ ç‡ä¼šä¹˜ä¸Š âˆš(height/width) å¾—åˆ°å¢å¼ºã€‚\n",
    "                scale = max(1, p.size(-2) / p.size(-1))**0.5\n",
    "                p.add_(g.view_as(p), alpha=-group[\"lr\"] * scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f8031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# æ•°æ®åŠ è½½ä¸ç¼“å­˜å‡½æ•°\n",
    "def load_and_cache_data(config: ModelConfig, cache_dir: str = \"data_cache\"):\n",
    "    \"\"\"åŠ è½½å¹¶ç¼“å­˜åˆ†è¯åçš„æ•°æ®ï¼Œä»¥é¿å…é‡å¤å¤„ç†\"\"\"\n",
    "    \n",
    "    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸€ä¸ª\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # æ ¹æ®é…ç½®ä¸­çš„æ–‡æ¡£æ•°é‡å’Œæœ€å¤§ Token æ•°ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„æ–‡ä»¶å\n",
    "    cache_file = f\"{cache_dir}/tokenized_data_{config.num_documents}_{config.max_tokens}.pkl\"\n",
    "\n",
    "    # --- æ­¥éª¤ 1ï¼šæ£€æŸ¥ç¼“å­˜ ---\n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"ğŸ“¦ æ­£åœ¨ä»ç¼“å­˜ä¸­åŠ è½½æ•°æ®: {cache_file}\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cached_data = pickle.load(f)\n",
    "\n",
    "        # ä»ç¼“å­˜ä¸­æå–æ•°æ®\n",
    "        texts = cached_data['texts']\n",
    "        tokenizer = cached_data['tokenizer']\n",
    "        tokens = cached_data['tokens']\n",
    "        # å°†åˆ†è¯å™¨çš„è¯è¡¨å¤§å°åŒæ­¥ç»™é…ç½®å¯¹è±¡\n",
    "        config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "        print(f\"âœ… åŠ è½½æˆåŠŸï¼šåŒ…å« {len(texts)} ç¯‡æ–‡æ¡£ï¼Œå…± {len(tokens):,} ä¸ª Token\")\n",
    "        return texts, tokenizer, tokens\n",
    "\n",
    "    # --- æ­¥éª¤ 2ï¼šå¤„ç†æ–°æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰ç¼“å­˜ï¼‰ ---\n",
    "    print(f\"ğŸ”„ æ­£åœ¨å¤„ç†æ–°æ•°æ®ï¼ˆå¤„ç†åå°†è‡ªåŠ¨ç¼“å­˜ä»¥å¤‡åç”¨ï¼‰\")\n",
    "\n",
    "    # 1. åŠ è½½åˆ†è¯å™¨ (ä½¿ç”¨ SmolLM é¢„è®­ç»ƒåˆ†è¯å™¨)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "    # å¦‚æœæ²¡æœ‰å®šä¹‰å¡«å……ç¬¦ï¼ˆPad Tokenï¼‰ï¼Œåˆ™ä½¿ç”¨ç»ˆæ­¢ç¬¦ï¼ˆEOS Tokenï¼‰ä»£æ›¿\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 2. ä» HuggingFace åŠ è½½æ•°æ®é›† (ä½¿ç”¨æµå¼åŠ è½½ streaming=Trueï¼ŒèŠ‚çœæ˜¾å­˜)\n",
    "    dataset = load_dataset(\"HuggingFaceTB/smollm-corpus\", \"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "    texts = []\n",
    "    # éå†æ•°æ®é›†ï¼Œåªæå–é…ç½®ä¸­æŒ‡å®šçš„æ–‡æ¡£æ•°é‡\n",
    "    for i, item in enumerate(dataset):\n",
    "        if i >= config.num_documents:\n",
    "            break\n",
    "        # æ¯ç¯‡æ–‡æ¡£æˆªå–å‰ 3000 ä¸ªå­—ç¬¦ï¼Œé˜²æ­¢å•æ¡æ•°æ®è¿‡å¤§\n",
    "        texts.append(item[\"text\"][:3000])\n",
    "\n",
    "    print(f\"å·²åŠ è½½ {len(texts)} ç¯‡åŸå§‹æ–‡æ¡£\")\n",
    "\n",
    "    # 3. æ‰§è¡Œåˆ†è¯æ“ä½œ\n",
    "    print(\"æ­£åœ¨è¿›è¡Œåˆ†è¯...\")\n",
    "    all_tokens = []\n",
    "    # ä½¿ç”¨ tqdm æ˜¾ç¤ºåˆ†è¯è¿›åº¦æ¡\n",
    "    for text in tqdm(texts, desc=\"åˆ†è¯è¿›åº¦\"):\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # 4. æˆªæ–­åˆ°é…ç½®è¦æ±‚çš„æœ€å¤§ Token æ•°é‡\n",
    "    tokens = all_tokens[:config.max_tokens]\n",
    "    print(f\"æœ€ç»ˆä½¿ç”¨ Token æ•°é‡: {len(tokens):,}\")\n",
    "    config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    # --- æ­¥éª¤ 3ï¼šä¿å­˜åˆ°ç¼“å­˜ ---\n",
    "    cached_data = {'texts': texts, 'tokenizer': tokenizer, 'tokens': tokens}\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(cached_data, f)\n",
    "\n",
    "    print(f\"ğŸ’¾ æ•°æ®å·²æˆåŠŸç¼“å­˜è‡³: {cache_file}\")\n",
    "    return texts, tokenizer, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea1001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenDataset(Dataset):\n",
    "    def __init__(self, tokens: List[int], seq_len: int = 512):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ•°æ®é›†\n",
    "        :param tokens: æ•´ä¸ªè¯­æ–™åº“åˆ†è¯åçš„ Token åˆ—è¡¨ï¼ˆä¸€ä¸ªå·¨å¤§çš„æ•´æ•°åˆ—è¡¨ï¼‰\n",
    "        :param seq_len: æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„åºåˆ—é•¿åº¦ï¼ˆçª—å£å¤§å°ï¼‰\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        è¿”å›æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°\n",
    "        è®¡ç®—æ–¹å¼ï¼šæ€» Token æ•°å‡å»åºåˆ—é•¿åº¦ã€‚\n",
    "        ä¾‹å¦‚ï¼šæœ‰ 100 ä¸ª tokenï¼Œçª—å£è®¾ä¸º 10ï¼Œåˆ™å¯ä»¥æ»‘åŠ¨å‡º 90 ä¸ªä¸åŒçš„èµ·ç‚¹ã€‚\n",
    "        ä½¿ç”¨ max(0, ...) æ˜¯ä¸ºäº†é˜²æ­¢ tokens é•¿åº¦å°äº seq_len æ—¶å‡ºç°è´Ÿæ•°å¯¼è‡´æŠ¥é”™ã€‚\n",
    "        \"\"\"\n",
    "        return max(0, len(self.tokens) - self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        æ ¹æ®ç»™å®šçš„ç´¢å¼• idx è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ (x, y)\n",
    "        x: è¾“å…¥åºåˆ—ï¼Œä» idx å¼€å§‹ï¼Œé•¿åº¦ä¸º seq_len\n",
    "        y: æ ‡ç­¾åºåˆ—ï¼ˆTargetï¼‰ï¼Œæ˜¯ x å¾€åæŒªåŠ¨ä¸€ä½çš„ç»“æœ\n",
    "        \"\"\"\n",
    "        # æå–è¾“å…¥åºåˆ— x: [tokens[idx], ..., tokens[idx + seq_len - 1]]\n",
    "        x = torch.tensor(self.tokens[idx : idx + self.seq_len], dtype=torch.long)\n",
    "        \n",
    "        # æå–ç›®æ ‡åºåˆ— y: [tokens[idx + 1], ..., tokens[idx + seq_len]]\n",
    "        # è¿™å°±æ˜¯æ‰€è°“çš„â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€ä»»åŠ¡ï¼Œæ¯ä¸€ä¸ªä½ç½®çš„ x å¯¹åº”çš„ y éƒ½æ˜¯å®ƒçš„ä¸‹ä¸€ä¸ªå­—ç¬¦\n",
    "        y = torch.tensor(self.tokens[idx + 1 : idx + self.seq_len + 1], dtype=torch.long)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cf2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– RoPE æ—‹è½¬ä½ç½®ç¼–ç \n",
    "        :param dim: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ (head_dim)\n",
    "        :param max_seq_len: æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- æ­¥éª¤ 1: è®¡ç®—è§’é¢‘ç‡ (Angular Frequencies) ---\n",
    "        # æŒ‰ç…§å…¬å¼è®¡ç®—é¢‘ç‡ï¼štheta_i = 10000^(-2i/dim)\n",
    "        # è¿™é‡Œ dim//4 æ˜¯å› ä¸º RoPE é€šå¸¸åªä½œç”¨äºå‰ä¸€åŠç»´åº¦ï¼Œä¸”æ¯ä¸¤ä¸ªç»´åº¦ç»„æˆä¸€ä¸ªæ—‹è½¬å¤æ•°å¹³é¢\n",
    "        angular_freq = (1 / 10000) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)\n",
    "        \n",
    "        # è¿™é‡Œå°†è®¡ç®—å‡ºçš„é¢‘ç‡ä¸ä¸€ç»„ 0 æ‹¼æ¥ï¼Œæ„å‘³ç€åªæœ‰å‰åŠéƒ¨åˆ†ç»´åº¦ä¼šè¿›è¡Œæ—‹è½¬\n",
    "        # è¿™ç§åšæ³•å¸¸è§äºæŸäº›é«˜æ•ˆå®ç°ï¼Œå¯ä»¥ä¿ç•™ä¸€éƒ¨åˆ†ç»´åº¦çš„ç»å¯¹ä½ç½®ä¿¡æ¯\n",
    "        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])\n",
    "        \n",
    "        # --- æ­¥éª¤ 2: æ„å»ºä½ç½®çŸ©é˜µ ---\n",
    "        # t æ˜¯ä½ç½®ç´¢å¼• [0, 1, 2, ..., max_seq_len-1]\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        \n",
    "        # ä½¿ç”¨å¤–ç§¯è®¡ç®—æ‰€æœ‰ä½ç½®çš„ theta å€¼: theta = t * angular_freq\n",
    "        # ç»“æœç»´åº¦ä¸º (max_seq_len, dim//2)\n",
    "        theta = torch.einsum(\"i,j -> ij\", t, angular_freq)\n",
    "        \n",
    "        # é¢„å…ˆè®¡ç®—å¹¶ç¼“å­˜ cos å’Œ sin å€¼ï¼Œæé«˜æ¨ç†æ•ˆç‡\n",
    "        # persistent=False è¡¨ç¤ºè¿™äº›ç¼“å­˜ä¸ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„ state_dict ä¸­\n",
    "        self.register_buffer('cos', theta.cos(), persistent=False)\n",
    "        self.register_buffer('sin', theta.sin(), persistent=False)\n",
    "\n",
    "    def forward(self, x_BTHD: torch.Tensor):\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œæ—‹è½¬å˜æ¢\n",
    "        :param x_BTHD: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (Batch, Time/SeqLen, Heads, head_dim)\n",
    "        \"\"\"\n",
    "        # æ£€æŸ¥ç¼“å­˜çš„ cos/sin é•¿åº¦æ˜¯å¦è¶³å¤Ÿè¦†ç›–å½“å‰è¾“å…¥åºåˆ—\n",
    "        assert self.cos.size(0) >= x_BTHD.size(-3)\n",
    "        \n",
    "        # æå–å½“å‰åºåˆ—é•¿åº¦å¯¹åº”çš„ cos å’Œ sinï¼Œå¹¶å¢åŠ ç»´åº¦ä»¥ä¾¿è¿›è¡Œå¹¿æ’­è®¡ç®—\n",
    "        # å½¢çŠ¶å˜ä¸º (1, SeqLen, 1, head_dim//2)\n",
    "        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]\n",
    "        \n",
    "        # å°†è¾“å…¥å¼ é‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆhead_dimï¼‰åˆ‡åˆ†ä¸ºä¸¤åŠ\n",
    "        x_BTHD_float = x_BTHD.to(dtype=torch.float32)\n",
    "        x1, x2 = x_BTHD_float.chunk(2, dim=-1)\n",
    "        \n",
    "        # --- æ­¥éª¤ 3: åº”ç”¨æ—‹è½¬çŸ©é˜µå˜æ¢ ---\n",
    "        # è¿™æ˜¯å¤æ•°ä¹˜æ³•çš„å®æ•°ç­‰ä»·å½¢å¼ï¼ˆæ—‹è½¬å˜æ¢å…¬å¼ï¼‰ï¼š\n",
    "        # y1 = x1 * cos + x2 * sin\n",
    "        # y2 = x2 * cos - x1 * sin\n",
    "        y1 = x1 * cos + x2 * sin\n",
    "        y2 = x1 * (-sin) + x2 * cos\n",
    "        \n",
    "        # å°†å¤„ç†åçš„ä¸¤éƒ¨åˆ†é‡æ–°æ‹¼æ¥ï¼Œå¹¶è½¬å›åŸå§‹æ•°æ®ç±»å‹ï¼ˆå¦‚ bfloat16ï¼‰\n",
    "        return torch.cat((y1, y2), dim=-1).type_as(x_BTHD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0c773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3Attention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_kv_heads = config.n_kv_heads\n",
    "        self.n_kv_groups = config.n_kv_groups\n",
    "        self.d_k = config.d_k\n",
    "\n",
    "        # ä¸º Q, K, V åˆ†åˆ«è®¾ç½®çº¿æ€§æŠ•å½±å±‚\n",
    "        self.q_proj = nn.Linear(self.d_model, self.n_heads * self.d_k, bias=config.attention_bias)\n",
    "        self.k_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
    "        self.v_proj = nn.Linear(self.d_model, self.n_kv_heads * self.d_k, bias=config.attention_bias)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "\n",
    "        # QK-Normalization å±‚ï¼šç”¨äºæé«˜è¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§\n",
    "        # è¿™ç§åšæ³•å¯ä»¥é˜²æ­¢ç‚¹ç§¯æ³¨æ„åŠ›çš„å€¼åœ¨è®­ç»ƒåˆæœŸæº¢å‡º\n",
    "        self.q_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
    "        self.k_norm = nn.RMSNorm(self.d_k, eps=config.rms_norm_eps)\n",
    "\n",
    "        # æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)\n",
    "        self.rotary = Rotary(self.d_k, config.max_seq_len)\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "\n",
    "        # 1. æ‰§è¡Œçº¿æ€§æŠ•å½±ï¼Œç”Ÿæˆ Q, K, V å‘é‡\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # 2. å°†è¾“å‡ºé‡å¡‘ï¼ˆReshapeï¼‰ä¸ºå¤šå¤´ç»“æ„\n",
    "        # å½¢çŠ¶å˜åŒ–: (batch, seq_len, d_model) -> (batch, seq_len, n_heads, d_k)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
    "        k = k.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "        v = v.view(batch_size, seq_len, self.n_kv_heads, self.d_k)\n",
    "\n",
    "        # 3. åº”ç”¨ QK-Norm\n",
    "        # åœ¨è¿›å…¥ç‚¹ç§¯è®¡ç®—å‰å¯¹ Q å’Œ K è¿›è¡Œå½’ä¸€åŒ–\n",
    "        q = self.q_norm(q)\n",
    "        k = self.k_norm(k)\n",
    "\n",
    "        # 4. åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œè¿›è¡Œäº†ç»´åº¦ç½®æ¢ï¼Œä»¥åŒ¹é… rotary æ¨¡å—çš„è¾“å…¥è¦æ±‚\n",
    "        # (batch, seq_len, n_heads, d_k) -> (batch, n_heads, seq_len, d_k)\n",
    "        q = self.rotary(q.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "        k = self.rotary(k.permute(0, 2, 1, 3)).permute(0, 2, 1, 3)\n",
    "\n",
    "        # ä¸ºæ³¨æ„åŠ›è®¡ç®—å‡†å¤‡å¼ é‡ï¼šå°† head ç»´åº¦ç§»åˆ°ç¬¬ 1 ç»´\n",
    "        Q = q.transpose(1, 2)\n",
    "        K = k.transpose(1, 2)\n",
    "        V = v.transpose(1, 2)\n",
    "\n",
    "        # 5. ä¸º GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰é‡å¤ K å’Œ V çš„å¤´\n",
    "        # å°† KV å¤´çš„æ•°é‡å¢åŠ åˆ°ä¸ Q å¤´ä¸€è‡´\n",
    "        K = repeat_kv(K, self.n_kv_groups)\n",
    "        V = repeat_kv(V, self.n_kv_groups)\n",
    "\n",
    "        # 6. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)\n",
    "        # ä½¿ç”¨ Flash Attention ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼ŒåŒæ—¶åº”ç”¨å› æœæ©ç  (is_causal=True)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            Q, K, V, is_causal=True, dropout_p=self.dropout if self.training else 0.0\n",
    "        )\n",
    "\n",
    "        # 7. é‡å¡‘å½¢çŠ¶å¹¶è¿›è¡Œæœ€åçš„çº¿æ€§è¾“å‡ºå˜æ¢\n",
    "        # å°†å¤šä¸ªå¤´çš„ç»“æœåˆå¹¶å› (batch, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.w_o(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a18eb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SwiGLUFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        SwiGLU å‰é¦ˆç¥ç»ç½‘ç»œæ¨¡å—\n",
    "        :param d_model: æ¨¡å‹çš„éšè—å±‚ç»´åº¦\n",
    "        :param d_ff: ä¸­é—´å±‚çš„ç»´åº¦ï¼ˆé€šå¸¸è®¾ä¸º d_model çš„ 2.7 åˆ° 4 å€ï¼‰\n",
    "        :param dropout: ä¸¢å¼ƒç‡\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # é—¨æ§æŠ•å½±å±‚ (Gate Projection)ï¼šè´Ÿè´£å†³å®šå“ªäº›ä¿¡æ¯å¯ä»¥é€šè¿‡\n",
    "        self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n",
    "        \n",
    "        # ç»´åº¦è¿˜åŸæŠ•å½±å±‚ (Down Projection)ï¼šå°†ç»´åº¦ä» d_ff è¿˜åŸå› d_model\n",
    "        self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n",
    "        \n",
    "        # ä¸Šé‡‡æ ·æŠ•å½±å±‚ (Up Projection)ï¼šè´Ÿè´£æå–æ›´é«˜ç»´åº¦çš„ç‰¹å¾\n",
    "        self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # SwiGLU æ¿€æ´»å‡½æ•°çš„å…·ä½“å®ç°é€»è¾‘ï¼š\n",
    "        # 1. å¯¹ gate_proj çš„è¾“å‡ºåº”ç”¨ SiLU (å³ Swish) æ¿€æ´»å‡½æ•°\n",
    "        # 2. å°†æ¿€æ´»åçš„ç»“æœä¸ up_proj çš„è¾“å‡ºè¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ï¼ˆHadamard Productï¼‰\n",
    "        # 3. è¿™ç§â€œé—¨æ§â€æœºåˆ¶èƒ½è®©æ¨¡å‹æ›´çµæ´»åœ°è¿‡æ»¤ç‰¹å¾\n",
    "        activated_x = F.silu(self.gate_proj(x)) * self.up_proj(x)\n",
    "        \n",
    "        # æœ€åè¿›è¡Œçº¿æ€§æŠ•å½±å¹¶åº”ç”¨ Dropout å¢å¼ºæ³›åŒ–èƒ½åŠ›\n",
    "        return self.down_proj(self.dropout(activated_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15123d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):  # Pass the entire config object\n",
    "        super().__init__()\n",
    "        self.attention = Qwen3Attention(config)\n",
    "        self.feed_forward = SwiGLUFeedForward(config.d_model, config.d_ff, config.dropout)\n",
    "        self.norm1 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.norm2 = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attention(self.norm1(x))\n",
    "        x = x + self.dropout(attn_out)\n",
    "        ff_out = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caecff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalLLM(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.output_dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "        # Tie weights\n",
    "        # This ties the output layer (`lm_head`) weights to the input token embedding weights so the model shares parameters between input and output, reducing memory and improving generalization.\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) * math.sqrt(self.config.d_model)\n",
    "        x = self.position_dropout(x)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.output_dropout(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58ec195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model: nn.Module, val_loader: DataLoader, config: ModelConfig):\n",
    "    \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
    "    # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼š\n",
    "    # è¿™ä¼šå…³é—­ Dropout å’Œ BatchNorm çš„éšæœºæ€§ï¼Œç¡®ä¿é¢„æµ‹ç»“æœæ˜¯ç¨³å®šçš„\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # è·å–æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼š\n",
    "    # è¯„ä¼°æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆä¸éœ€è¦åå‘ä¼ æ’­ï¼‰ï¼Œè¿™æ ·å¯ä»¥å¤§å¹…èŠ‚çœæ˜¾å­˜å¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            # å¦‚æœè¾¾åˆ°äº†é…ç½®ä¸­æŒ‡å®šçš„è¯„ä¼°æ­¥æ•°ï¼Œåˆ™åœæ­¢ï¼Œä»¥é™åˆ¶è¯„ä¼°æ—¶é—´\n",
    "            if i >= config.eval_steps:\n",
    "                break\n",
    "\n",
    "            # å°†è¾“å…¥åºåˆ— (x) å’Œç›®æ ‡åºåˆ— (y) ç§»åŠ¨åˆ° GPU/è®¾å¤‡ä¸Š\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # å¦‚æœå¯ç”¨äº†æ··åˆç²¾åº¦è®­ç»ƒ (AMP)ï¼Œè¿™é‡Œä¹Ÿä¼šåŒæ­¥ä½¿ç”¨\n",
    "            # è¿™å¯ä»¥åœ¨ä¿è¯ç²¾åº¦çš„å‰æä¸‹æé«˜æ¨ç†é€Ÿåº¦\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # å‰å‘ä¼ æ’­ï¼šè·å–æ¨¡å‹å¯¹è¾“å…¥åºåˆ—çš„é¢„æµ‹ç»“æœï¼ˆLogitsï¼‰\n",
    "                logits = model(x)\n",
    "\n",
    "                # è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy Lossï¼‰ï¼š\n",
    "                # éœ€è¦å°†å½¢çŠ¶å±•å¹³ï¼š(batch_size * seq_len, vocab_size) å’Œ (batch_size * seq_len)\n",
    "                # è¿™æ ·æ‰èƒ½å¯¹æ‰€æœ‰ä½ç½®çš„ Token è¿›è¡Œç»Ÿä¸€çš„æŸå¤±è®¡ç®—\n",
    "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "\n",
    "            # ç´¯åŠ æ€»æŸå¤±ï¼šä½¿ç”¨ loss.item() ä¹˜ä»¥å½“å‰ Batch çš„ Token æ€»æ•°\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            # è®°å½•å¤„ç†è¿‡çš„ Token æ€»æ•°\n",
    "            total_tokens += y.numel()\n",
    "\n",
    "            # è·å–é¢„æµ‹ç»“æœï¼šåœ¨è¯è¡¨ç»´åº¦ï¼ˆæœ€åä¸€ç»´ï¼‰å–æœ€å¤§å€¼çš„ç´¢å¼•ï¼ˆArgmaxï¼‰\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            # ç»Ÿè®¡é¢„æµ‹æ­£ç¡®çš„ Token æ•°é‡ï¼Œç”¨äºè®¡ç®—å‡†ç¡®ç‡\n",
    "            total_correct += (predictions == y).sum().item()\n",
    "\n",
    "    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    accuracy = total_correct / total_tokens\n",
    "    \n",
    "    # è®¡ç®—å›°æƒ‘åº¦ (Perplexity)ï¼šå®ƒæ˜¯ Loss çš„æŒ‡æ•°å½¢å¼ï¼Œä»£è¡¨æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶çš„â€œè¿·èŒ«ç¨‹åº¦â€\n",
    "    # é™åˆ¶æœ€å¤§å€¼ä¸º exp(20) ä»¥é˜²æ­¢æº¢å‡º\n",
    "    perplexity = math.exp(min(avg_loss, 20))\n",
    "\n",
    "    # è€ƒè¯•ç»“æŸï¼Œå°†æ¨¡å‹é‡æ–°åˆ‡å›è®­ç»ƒæ¨¡å¼ï¼Œå‡†å¤‡æ¥ä¸‹æ¥çš„è®­ç»ƒ\n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'val_loss': avg_loss, \n",
    "        'val_accuracy': accuracy, \n",
    "        'val_perplexity': perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a833ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_muon_optimizer(model: nn.Module, config: ModelConfig):\n",
    "    \"\"\"Setup Muon optimizer with hybrid approach\"\"\"\n",
    "    muon_params = []\n",
    "    adamw_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if (param.ndim == 2 and\n",
    "            'token_embedding' not in name and\n",
    "            'norm' not in name and\n",
    "            param.requires_grad):\n",
    "            muon_params.append(param)\n",
    "        else:\n",
    "            adamw_params.append(param)\n",
    "\n",
    "    print(f\"  Muon parameters: {sum(p.numel() for p in muon_params):,}\")\n",
    "    print(f\"  AdamW parameters: {sum(p.numel() for p in adamw_params):,}\")\n",
    "\n",
    "    muon_optimizer = Muon(muon_params, lr=config.muon_lr, momentum=0.95)\n",
    "    adamw_optimizer = torch.optim.AdamW(adamw_params, lr=config.muon_lr*0.1, weight_decay=config.weight_decay)\n",
    "\n",
    "    return [muon_optimizer, adamw_optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "780028b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: ModelConfig, train_loader: DataLoader, val_loader: DataLoader):\n",
    "    \"\"\"ä½¿ç”¨ Muon ä¼˜åŒ–å™¨è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    print(f\"\\nğŸš€ å¼€å§‹è®­ç»ƒ (æ¶æ„: Modern LLM, ä¼˜åŒ–å™¨: Muon)\")\n",
    "\n",
    "    # 1. åˆå§‹åŒ–æ¨¡å‹ä¸ç¯å¢ƒ\n",
    "    set_seed(42) # å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°\n",
    "    model = MinimalLLM(config)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "\n",
    "    # 2. è®¾ç½®ä¼˜åŒ–å™¨ï¼šè¿™é‡Œä¼šè¿”å› [muon_optimizer, adamw_optimizer] ä¸¤ä¸ªå¯¹è±¡\n",
    "    optimizers = setup_muon_optimizer(model, config)\n",
    "\n",
    "    # 3. è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ (Learning Rate Schedule)\n",
    "    # ä½¿ç”¨ Cosine Decay (ä½™å¼¦é€€ç«) ç­–ç•¥ï¼Œå¹¶å¸¦æœ‰ Warmup (é¢„çƒ­) é˜¶æ®µ\n",
    "    schedulers = []\n",
    "    for optimizer in optimizers:\n",
    "        warmup_steps = config.max_steps // 20 # å‰ 5% çš„æ­¥æ•°ç”¨äºé¢„çƒ­\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            # é¢„çƒ­é˜¶æ®µï¼šå­¦ä¹ ç‡ä» 0 çº¿æ€§å¢åŠ åˆ°è®¾å®šå€¼\n",
    "            if step < warmup_steps:\n",
    "                return step / warmup_steps\n",
    "            # é€€ç«é˜¶æ®µï¼šå­¦ä¹ ç‡æŒ‰ä½™å¼¦æ›²çº¿ä¸‹é™è‡³åˆå§‹å€¼çš„ 10%\n",
    "            else:\n",
    "                progress = (step - warmup_steps) / (config.max_steps - warmup_steps)\n",
    "                return 0.1 + 0.9 * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "            # ä½¿ç”¨ LambdaLR çµæ´»æ§åˆ¶å­¦ä¹ ç‡\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        schedulers.append(scheduler)\n",
    "\n",
    "    # 4. å‡†å¤‡æ··åˆç²¾åº¦è®­ç»ƒ (AMP)\n",
    "    # GradScaler ç”¨äºé˜²æ­¢ FP16 è®­ç»ƒæ—¶å› æ¢¯åº¦è¿‡å°å¯¼è‡´çš„æ•°å€¼ä¸‹æº¢\n",
    "    scaler = GradScaler() if config.use_amp else None\n",
    "\n",
    "    # 5. è®­ç»ƒçŠ¶æ€åˆå§‹åŒ–\n",
    "    model.train()\n",
    "    step = 0\n",
    "    start_time = time.time()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "    pbar = tqdm(total=config.max_steps, desc=\"æ­£åœ¨è®­ç»ƒ\")\n",
    "\n",
    "    while step < config.max_steps:\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            if step >= config.max_steps:\n",
    "                break\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # --- å‰å‘ä¼ æ’­ä¸æ¢¯åº¦ç´¯ç§¯ ---\n",
    "            # æ¢¯åº¦ç´¯ç§¯å…è®¸æˆ‘ä»¬åœ¨æ˜¾å­˜æœ‰é™çš„æƒ…å†µä¸‹ï¼Œæ¨¡æ‹Ÿå‡ºæ›´å¤§çš„ Batch Size\n",
    "            if config.use_amp:\n",
    "                with autocast(): # å¼€å¯è‡ªåŠ¨æ··åˆç²¾åº¦\n",
    "                    logits = model(x)\n",
    "                    loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "                    # å°† Loss é™¤ä»¥ç´¯ç§¯æ­¥æ•°ï¼Œå› ä¸ºæ¢¯åº¦æ˜¯ç´¯åŠ çš„\n",
    "                    loss = loss / config.gradient_accumulation_steps\n",
    "                scaler.scale(loss).backward() # ç¼©æ”¾ Loss å¹¶åå‘ä¼ æ’­\n",
    "            else:\n",
    "                logits = model(x)\n",
    "                loss = F.cross_entropy(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "                loss = loss / config.gradient_accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "            # --- ä¼˜åŒ–å™¨æ›´æ–°æ­¥ (åªæœ‰è¾¾åˆ°ç´¯ç§¯æ­¥æ•°æ—¶æ‰æ‰§è¡Œ) ---\n",
    "            if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "                if config.use_amp:\n",
    "                    # åœ¨æ›´æ–°å‰å…ˆ unscale æ¢¯åº¦ï¼Œä»¥ä¾¿è¿›è¡Œæ¢¯åº¦è£å‰ª\n",
    "                    for optimizer in optimizers:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    \n",
    "                    # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç»´æŒè®­ç»ƒç¨³å®šæ€§\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "\n",
    "                    # åˆ†åˆ«æ›´æ–°ä¸¤ä¸ªä¼˜åŒ–å™¨ï¼ˆMuon å’Œ AdamWï¼‰\n",
    "                    for optimizer in optimizers:\n",
    "                        scaler.step(optimizer)\n",
    "                        optimizer.zero_grad() # æ¸…ç©ºæ¢¯åº¦ï¼Œä¸ºä¸‹ä¸€è½®åšå‡†å¤‡\n",
    "                    for scheduler in schedulers:\n",
    "                        scheduler.step() # æ›´æ–°å­¦ä¹ ç‡\n",
    "                    scaler.update() # æ›´æ–° Scaler çš„ç¼©æ”¾å› å­\n",
    "                else:\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                    for optimizer in optimizers:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    for scheduler in schedulers:\n",
    "                        scheduler.step()\n",
    "\n",
    "            # 6. æ—¥å¿—è®°å½• (æ¯ 10 æ­¥è®°å½•ä¸€æ¬¡)\n",
    "            if step % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    predictions = logits.argmax(dim=-1)\n",
    "                    accuracy = (predictions == y).float().mean().item()\n",
    "                    # æ¢å¤çœŸå®çš„ Loss å€¼ç”¨äºæ˜¾ç¤º\n",
    "                    current_loss = loss.item() * config.gradient_accumulation_steps\n",
    "                    perplexity = math.exp(min(current_loss, 20))\n",
    "\n",
    "                # åœ¨è¿›åº¦æ¡å³ä¾§å®æ—¶æ›´æ–°å…³é”®æŒ‡æ ‡\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{current_loss:.4f}',\n",
    "                    'acc': f'{accuracy:.3f}',\n",
    "                    'ppl': f'{perplexity:.1f}',\n",
    "                    'lr': f'{optimizers[0].param_groups[0][\"lr\"]:.2e}' # æ˜¾ç¤º Muon çš„å­¦ä¹ ç‡\n",
    "                })\n",
    "\n",
    "            # 7. å®šæœŸè¿›è¡ŒéªŒè¯ (Evaluation)\n",
    "            if step % config.eval_every == 0 and step > 0:\n",
    "                eval_metrics = evaluate_model(model, val_loader, config)\n",
    "                print(f\"\\næ­¥æ•° {step}: éªŒè¯æŸå¤±: {eval_metrics['val_loss']:.4f}, \"\n",
    "                      f\"éªŒè¯å‡†ç¡®ç‡: {eval_metrics['val_accuracy']:.4f}, \"\n",
    "                      f\"éªŒè¯å›°æƒ‘åº¦: {eval_metrics['val_perplexity']:.2f}\")\n",
    "\n",
    "                # å¦‚æœéªŒè¯æŸå¤±ç ´äº†çºªå½•ï¼Œåˆ™ä¿å­˜â€œæœ€ä½³æ¨¡å‹â€\n",
    "                if eval_metrics['val_loss'] < best_val_loss:\n",
    "                    best_val_loss = eval_metrics['val_loss']\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'config': config,\n",
    "                        'step': step,\n",
    "                        'best_val_loss': best_val_loss,\n",
    "                        'final_metrics': eval_metrics\n",
    "                    }, 'best_model.pt')\n",
    "                    print(f\"ğŸ’¾ å·²ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå½“å‰éªŒè¯æŸå¤±: {best_val_loss:.4f}\")\n",
    "\n",
    "            step += 1\n",
    "            if step % 10 == 0:\n",
    "                pbar.update(10)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # 8. è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"  â±ï¸ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {training_time:.1f} ç§’\")\n",
    "\n",
    "    final_eval = evaluate_model(model, val_loader, config)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'step': step,\n",
    "        'final_metrics': final_eval\n",
    "    }, 'final_model.pt')\n",
    "    print(f\"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆæ¨¡å‹è‡³ final_model.pt\")\n",
    "\n",
    "    return model, final_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa065435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” è®¾å¤‡æ£€æµ‹: CUDA\n",
      "GPU å‹å·: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "æ˜¾å­˜å¤§å°: 12.9 GB\n",
      "Set all seed to 42\n",
      "\n",
      "ğŸ“‹ æ¨¡å‹é…ç½®æ¦‚è§ˆ:\n",
      "   æ¶æ„å‚æ•°: 384d, 6L, 8H, 1536ff\n",
      "   è®­ç»ƒå‚æ•°: å…± 500 æ­¥, Batch Size 24\n",
      "   æ•°æ®è§„æ¨¡: ä¸Šé™ 500,000 Token, æœ€å¤§åºåˆ—é•¿åº¦ 256\n",
      "ğŸ“¦ æ­£åœ¨ä»ç¼“å­˜ä¸­åŠ è½½æ•°æ®: data_cache/tokenized_data_2000_500000.pkl\n",
      "âœ… åŠ è½½æˆåŠŸï¼šåŒ…å« 2000 ç¯‡æ–‡æ¡£ï¼Œå…± 500,000 ä¸ª Token\n",
      "ğŸ“Š æ•°æ®é›†åˆ†å¸ƒ: 449770 è®­ç»ƒæ ·æœ¬, 49974 éªŒè¯æ ·æœ¬\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 1. ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # æ£€æŸ¥å½“å‰è®¾å¤‡æ˜¯å¦æ”¯æŒ CUDA (Nvidia GPU)ï¼Œå¦‚æœä¸è¡Œåˆ™å›é€€åˆ° CPU\n",
    "    print(f\"ğŸ” è®¾å¤‡æ£€æµ‹: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        # æ‰“å°æ˜¾å¡å‹å·ï¼Œç¡®è®¤æ²¡æœ‰è·‘åœ¨æ ¸æ˜¾æˆ–è€…é”™è¯¯çš„å¡ä¸Š\n",
    "        print(f\"GPU å‹å·: {torch.cuda.get_device_name()}\")\n",
    "        # æ‰“å°æ˜¾å­˜å¤§å°ï¼Œç¡®è®¤æ˜¯å¦æœ‰è¶³å¤Ÿæ˜¾å­˜è·‘å¤§æ¨¡å‹ (1e9 = 1GB)\n",
    "        print(f\"æ˜¾å­˜å¤§å°: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 2. éšæœºç§å­è®¾ç½®\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # é”å®šéšæœºç§å­ (Seed) æ˜¯å¤ç°å®éªŒç»“æœçš„å…³é”®ã€‚\n",
    "    # å®ƒå¯ä»¥ä¿è¯æ¯æ¬¡è¿è¡Œä»£ç æ—¶ï¼Œå‚æ•°åˆå§‹åŒ–å’Œæ•°æ®æ‰“ä¹±çš„é¡ºåºéƒ½æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚\n",
    "    set_seed(42)\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 3. åˆå§‹åŒ–æ¨¡å‹é…ç½®\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # å®ä¾‹åŒ–é…ç½®ç±»ï¼Œè¿™é‡Œå®šä¹‰äº†æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°ï¼ˆå±‚æ•°ã€ç»´åº¦ç­‰ï¼‰\n",
    "    config = ModelConfig()\n",
    "    print(f\"\\nğŸ“‹ æ¨¡å‹é…ç½®æ¦‚è§ˆ:\")\n",
    "    # d: ç»´åº¦ (dimension), L: å±‚æ•° (layers), H: å¤´æ•° (heads), ff: å‰é¦ˆç½‘ç»œç»´åº¦\n",
    "    print(f\"   æ¶æ„å‚æ•°: {config.d_model}d, {config.n_layers}L, {config.n_heads}H, {config.d_ff}ff\")\n",
    "    print(f\"   è®­ç»ƒå‚æ•°: å…± {config.max_steps} æ­¥, Batch Size {config.batch_size}\")\n",
    "    print(f\"   æ•°æ®è§„æ¨¡: ä¸Šé™ {config.max_tokens:,} Token, æœ€å¤§åºåˆ—é•¿åº¦ {config.max_seq_len}\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 4. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # åŠ è½½æ–‡æœ¬æ•°æ®å¹¶è¿›è¡Œ Tokenizationï¼ˆåˆ†è¯ï¼‰ã€‚é€šå¸¸è¿™ä¸€æ­¥ä¼šæŠŠæ–‡æœ¬è½¬æˆæ•°å­— ID åˆ—è¡¨ã€‚\n",
    "    texts, tokenizer, tokens = load_and_cache_data(config)\n",
    "    \n",
    "    # å°è£…æˆ PyTorch çš„ Dataset å¯¹è±¡ï¼Œè´Ÿè´£æŒ‰ max_seq_len åˆ‡å‰²æ•°æ®\n",
    "    dataset = TextTokenDataset(tokens, config.max_seq_len)\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. æ•°æ®é›†åˆ’åˆ† (Train/Val Split)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # æŒ‰ç…§ 9:1 çš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    # éªŒè¯é›† (Val) ç”¨æ¥åœ¨è®­ç»ƒä¸­é€”æµ‹è¯•æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆ\n",
    "    val_size = len(dataset) // 10\n",
    "    train_size = len(dataset) - val_size\n",
    "    \n",
    "    # ä½¿ç”¨ random_split è¿›è¡Œéšæœºåˆ‡å‰²ï¼ŒåŒæ ·å›ºå®š generator ç§å­ä»¥ä¿è¯å¯å¤ç°\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # åˆ›å»º DataLoaderï¼Œè´Ÿè´£æŒ‰ Batch æ‰¹é‡è¾“é€æ•°æ®\n",
    "    # shuffle=True: è®­ç»ƒé›†å¿…é¡»æ‰“ä¹±ï¼Œé˜²æ­¢æ¨¡å‹è®°ä½æ•°æ®çš„é¡ºåº\n",
    "    # num_workers=2: å¯ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®ã€‚æ³¨æ„ï¼šåœ¨ Windows ä¸Šå¦‚æœä½ é‡åˆ°æŠ¥é”™ï¼Œå¯èƒ½éœ€è¦æ”¹ä¸º 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"ğŸ“Š æ•°æ®é›†åˆ†å¸ƒ: {len(train_dataset)} è®­ç»ƒæ ·æœ¬, {len(val_dataset)} éªŒè¯æ ·æœ¬\")\n",
    "\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 6. å¼€å§‹è®­ç»ƒ\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # start_time = time.time()\n",
    "    \n",
    "    # # è°ƒç”¨æ ¸å¿ƒè®­ç»ƒå‡½æ•°ï¼Œå¼€å§‹ç‚¼ä¸¹ï¼\n",
    "    # # è¿”å›è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡å’Œæœ€ç»ˆçš„è¯„ä¼°æŒ‡æ ‡\n",
    "    # model, final_metrics = train_model(config, train_loader, val_loader)\n",
    "    \n",
    "    # total_time = time.time() - start_time\n",
    "\n",
    "    # # -----------------------------------------------------------------------------\n",
    "    # # 7. è®­ç»ƒæ€»ç»“ä¸æ±‡æŠ¥\n",
    "    # # -----------------------------------------------------------------------------\n",
    "    # print(f\"\\nğŸ‰ è®­ç»ƒåœ†æ»¡å®Œæˆ!\")\n",
    "    # print(f\"â±ï¸ æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ\")\n",
    "    # print(f\"ğŸ† æœ€ç»ˆæˆç»©å•:\")\n",
    "    # print(f\"   éªŒè¯æŸå¤± (Loss): {final_metrics['val_loss']:.4f} (è¶Šä½è¶Šå¥½)\")\n",
    "    # print(f\"   éªŒè¯å‡†ç¡®ç‡ (Acc): {final_metrics['val_accuracy']:.4f} (è¶Šé«˜è¶Šå¥½)\")\n",
    "    # # å›°æƒ‘åº¦ (Perplexity) æ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æœ¬è‡ªç„¶ç¨‹åº¦çš„æ ¸å¿ƒæŒ‡æ ‡\n",
    "    # # 20 å·¦å³å¯¹äºå°æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªéå¸¸ä¸é”™çš„æˆç»©\n",
    "    # print(f\"   éªŒè¯å›°æƒ‘åº¦ (PPL): {final_metrics['val_perplexity']:.2f} (è¶Šä½è¶Šå¥½)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leeml23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
