import os
import glob
import numpy as np
import tiktoken
from datasets import load_dataset
from tqdm import tqdm
import multiprocessing as mp

# --- 1. é…ç½®å‚æ•° ---
# å­˜å‚¨å¤„ç†å Token çš„ç›®å½•
local_dir = "edu_fineweb10B"
# å­˜å‚¨åŸå§‹ä¸‹è½½çš„ Parquet æ–‡ä»¶çš„ç›®å½•
raw_data_dir = "./raw_data/sample-10BT"
# æ¯ä¸ªåˆ†ç‰‡çš„ Token æ•°é‡ (100M)
shard_size = int(1e8) 

DATA_CACHE_DIR = os.path.join(os.path.dirname(__file__), local_dir)
os.makedirs(DATA_CACHE_DIR, exist_ok=True)

# åˆå§‹åŒ–åˆ†è¯å™¨
enc = tiktoken.get_encoding("gpt2")
eot = enc._special_tokens['<|endoftext|>'] 

def tokenize(doc):
    """å•ä¸ªæ–‡æ¡£çš„åˆ†è¯å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œè°ƒç”¨"""
    tokens = [eot]
    tokens.extend(enc.encode_ordinary(doc["text"]))
    tokens_np = np.array(tokens)
    # ç¡®ä¿åœ¨ uint16 èŒƒå›´å†…
    tokens_np = np.clip(tokens_np, 0, 2**16 - 1)
    return tokens_np.astype(np.uint16)

def write_shard(filename, tokens_np):
    """å†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶"""
    tokens_np.tofile(filename)
    print(f"\nâœ… Saved {filename}")

def main():
    # æ‰«ææœ¬åœ°æ–‡ä»¶
    data_files = sorted(glob.glob(os.path.join(raw_data_dir, "*.parquet")))
    if not data_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {raw_data_dir} ä¸‹æ²¡æ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä¸‹è½½è·¯å¾„ã€‚")
        return

    print(f"ğŸš€ æ‰¾åˆ° {len(data_files)} ä¸ªæœ¬åœ°æ–‡ä»¶ï¼Œå‡†å¤‡ä½¿ç”¨ {mp.cpu_count()} ä¸ªæ ¸å¿ƒå¹¶è¡Œå¤„ç†...")

    # ä½¿ç”¨æœ¬åœ° parquet åŠ è½½
    fw = load_dataset("parquet", data_files=data_files, split="train", streaming=True)

    shard_index = 0
    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)
    token_count = 0
    
    # ä½¿ç”¨è¿›ç¨‹æ± åŠ é€Ÿåˆ†è¯
    # æˆ‘ä»¬åˆ†æ‰¹æ¬¡å¤„ç†æ–‡æ¡£ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
    with mp.Pool(mp.cpu_count()) as pool:
        # ä½¿ç”¨ imap ä¿æŒé¡ºåºå¹¶æµå¼å¤„ç†
        for tokens in tqdm(pool.imap(tokenize, fw, chunksize=16), desc="Processing Tokens", unit="docs"):
            
            # å¦‚æœå½“å‰åˆ†ç‰‡æ”¾å¾—ä¸‹
            if token_count + len(tokens) < shard_size:
                all_tokens_np[token_count : token_count + len(tokens)] = tokens
                token_count += len(tokens)
            else:
                # åˆ†ç‰‡å·²æ»¡ï¼Œå†™å…¥æ–‡ä»¶
                split = "val" if shard_index == 0 else "train"
                filename = os.path.join(DATA_CACHE_DIR, f"edufineweb_{split}_{shard_index:06d}")
                
                # å¡«æ»¡å½“å‰åˆ†ç‰‡
                remainder = shard_size - token_count
                all_tokens_np[token_count : shard_size] = tokens[:remainder]
                write_shard(filename, all_tokens_np)
                
                # å‡†å¤‡ä¸‹ä¸€ä¸ªåˆ†ç‰‡
                shard_index += 1
                # å°†å‰©ä½™ token æ”¾å…¥æ–° buffer
                leftover = tokens[remainder:]
                if len(leftover) > 0:
                    all_tokens_np[0 : len(leftover)] = leftover
                    token_count = len(leftover)
                else:
                    token_count = 0

    # å†™å…¥æœ€åä¸€ä¸ªä¸æ»¡çš„åˆ†ç‰‡
    if token_count > 0:
        split = "val" if shard_index == 0 else "train"
        filename = os.path.join(DATA_CACHE_DIR, f"edufineweb_{split}_{shard_index:06d}")
        write_shard(filename, all_tokens_np[:token_count])

    print("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼å¯ä»¥å¼€å§‹ pretrain äº†ã€‚")

if __name__ == "__main__":
    main()